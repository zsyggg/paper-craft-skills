# Engram 深度解析 - storytelling 风格

**论文标题**：Conditional Memory via Scalable Lookup
**作者团队**：北京大学 & DeepSeek-AI

---

## 大模型其实没有"记忆"

你有没有想过这个问题：当我们问 ChatGPT "中国的首都在哪里"，它是怎么知道答案的？

直觉上，我们可能会觉得模型"记住"了这个知识。但实际上，大模型根本没有这样的"记忆模块"——它所有的答案都是**算**出来的。

当你输入"中国的首都在"这几个字，模型会启动几十层 Transformer，几十亿次浮点数运算之后，才会输出"北京"。

这种方式有个明显的问题：**大炮打蚊子**。

我们人脑可不是这么工作的。人脑有专门的海马体负责记忆，记忆就是记忆，计算就是计算，各司其职。

那么问题来了：能不能给大模型也加上一个"记忆模块"？

## 从 N-gram 说起

要理解 Engram，我们得先回顾一个古老的技术：N-gram 语言模型。

在深度学习之前，人们是怎么做语言模型的？很简单——统计。

比如 1-gram 会统计"中"后面跟什么字的频率。但这个模型太蠢了，只看一个字根本不够。

2-gram 看前两个字，"都在"后面跟"北"的概率就很高了。但 3-gram、4-gram 呢？词汇表会爆炸，根本存不下。

Engram 的核心思想就是：**把 N-gram 的查表优势和 Transformer 的计算能力结合起来**。

## 记忆归记忆，计算归计算

Engram 在 Transformer 的特定层插入一个"条件记忆模块"：

1. **提取 N-gram 并查表**：用哈希函数把 N-gram 映射到嵌入表，O(1) 复杂度
2. **上下文门控**：用当前隐藏状态决定"要不要信这条记忆"
3. **融合到主干**：门控后的记忆通过残差连接加到主干网络

整个设计的精髓在于：静态的模式通过查表获取，动态的推理交给 Transformer。

## 实验结果

等参数下，Engram-27B 相比 MoE-27B：
- MMLU +3.4, CMMLU +4.0（知识任务）
- BBH +5.0, ARC-Challenge +3.7（推理任务）
- 长上下文 84.2→97.0（惊人提升）

一句话总结：**记忆归记忆，计算归计算**。
