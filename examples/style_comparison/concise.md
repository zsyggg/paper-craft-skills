# Engram 深度解析 - concise 风格

**论文**：Conditional Memory via Scalable Lookup
**团队**：北京大学 & DeepSeek-AI

---

## 核心创新

Transformer 中引入 O(1) 查表机制，将静态知识从计算中分离。

## 关键设计

| 组件 | 功能 |
|------|------|
| N-gram 哈希检索 | 多头哈希映射到嵌入表 |
| 上下文门控 | 隐藏状态决定是否采纳记忆 |
| 确定性索引 | 支持预取，推理开销 <3% |

## 最优配置

MoE : Engram = 75-80% : 20-25%

## 主要结果

等参数下 vs MoE-27B：
- **MMLU** +3.4
- **BBH** +5.0
- **长上下文** 84.2→97.0

## 一句话总结

记忆归记忆，计算归计算。
