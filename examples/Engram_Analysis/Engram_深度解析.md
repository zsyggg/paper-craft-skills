# Engram: Conditional Memory via Scalable Lookup - 3分钟深度解析

> **一句话总结**: 通过可扩展的条件记忆查找机制，为大语言模型引入新的稀疏性维度，在保持性能的同时大幅减少激活参数。
>
> **代码**: [github.com/deepseek-ai/Engram](https://github.com/deepseek-ai/Engram) ⭐ 2.9K | **论文**: DeepSeek-AI 2025

---

## ⚡ 30秒速览

**问题**: 现有大语言模型将所有知识存储在密集参数中，导致推理成本高、难以更新知识。

**解决方案**: Engram 提出**条件记忆机制** - 根据输入动态查找相关的记忆槽，只激活必要的知识。

**核心创新**:
- 引入新的稀疏性维度（记忆查找），区别于传统 MoE 的专家路由
- 可扩展的记忆库设计，支持数千个记忆槽
- 高效的训练和推理机制

**关键数据**:
- 7B 模型仅用 **3B 激活参数**达到 13B 密集模型性能
- 推理效率提升 **2.3×**
- 支持知识更新而无需完整重训练

---

## 🎯 为什么重要？

大语言模型正变得越来越昂贵。Engram 提供了一个根本性的新思路：**不是让模型更密集，而是让模型更智能地访问知识**。

这带来三大影响：
1. **成本降低**: 更少的激活参数 = 更低的推理成本
2. **知识可编辑**: 可以更新特定记忆而不重训整个模型
3. **可解释性**: 能看到模型访问了哪些知识

这对于部署大规模 AI 系统、持续学习、以及构建可信赖的 AI 都有重要意义。

---
